{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\"Ashraf Marwan is remembered most famously for spying for the Egyptian intelligence agency\",\n",
    "             \"feeding Egypt strategic information on the location of Israeli military assets\",\n",
    "             \"Marwans unparalleled access to his nations best kept secrets especially after his promotion\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['is', 'of', 'the', 'for', 'to', 'on', 'his']\n",
    "\n",
    "sentences = []\n",
    "for text in documents:\n",
    "    text = text.lower()\n",
    "    tmp = text.split(\" \")\n",
    "    text_clean = [w for w in tmp if not w in stop_words]\n",
    "    sentences.append(\" \".join(text_clean))\n",
    "    \n",
    "\n",
    "words = []\n",
    "for docs in sentences:\n",
    "    tokenized = docs.split()\n",
    "    for token_words in tokenized:\n",
    "        if not token_words in stop_words:\n",
    "            words.append(token_words)        \n",
    "\n",
    "# unique words in corpus\n",
    "unique_words = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ashraf marwan remembered most famously spying egyptian intelligence agency',\n",
       " 'feeding egypt strategic information location israeli military assets',\n",
       " 'marwans unparalleled access nations best kept secrets especially after promotion']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'access',\n",
       " 'after',\n",
       " 'agency',\n",
       " 'ashraf',\n",
       " 'assets',\n",
       " 'best',\n",
       " 'egypt',\n",
       " 'egyptian',\n",
       " 'especially',\n",
       " 'famously',\n",
       " 'feeding',\n",
       " 'information',\n",
       " 'intelligence',\n",
       " 'israeli',\n",
       " 'kept',\n",
       " 'location',\n",
       " 'marwan',\n",
       " 'marwans',\n",
       " 'military',\n",
       " 'most',\n",
       " 'nations',\n",
       " 'promotion',\n",
       " 'remembered',\n",
       " 'secrets',\n",
       " 'spying',\n",
       " 'strategic',\n",
       " 'unparalleled'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning word int value\n",
    "idx = 1\n",
    "word_ids = {}\n",
    "for idx, val in enumerate(unique_words):\n",
    "    word_ids[val] = idx    # if not +1 we'll get first element be 0           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ashraf marwan remembered most famously spying egyptian intelligence agency',\n",
       " 'feeding egypt strategic information location israeli military assets',\n",
       " 'marwans unparalleled access nations best kept secrets especially after promotion']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feeding': 0,\n",
       " 'marwan': 1,\n",
       " 'location': 2,\n",
       " 'information': 3,\n",
       " 'spying': 4,\n",
       " 'nations': 5,\n",
       " 'marwans': 6,\n",
       " 'best': 7,\n",
       " 'israeli': 8,\n",
       " 'especially': 9,\n",
       " 'egypt': 10,\n",
       " 'most': 11,\n",
       " 'intelligence': 12,\n",
       " 'secrets': 13,\n",
       " 'egyptian': 14,\n",
       " 'kept': 15,\n",
       " 'access': 16,\n",
       " 'unparalleled': 17,\n",
       " 'ashraf': 18,\n",
       " 'famously': 19,\n",
       " 'remembered': 20,\n",
       " 'strategic': 21,\n",
       " 'assets': 22,\n",
       " 'military': 23,\n",
       " 'promotion': 24,\n",
       " 'after': 25,\n",
       " 'agency': 26}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting labels/Neighbours using skip gram \n",
    "main_word = []\n",
    "lb_word = []   # neighbour word\n",
    "\n",
    "WINDOW_SIZE = 2\n",
    "# print(sentences)\n",
    "\n",
    "tok_sentences = []\n",
    "for wrds in sentences:\n",
    "    tok_sentences.append(wrds.split())\n",
    "\n",
    "for sentence in tok_sentences:\n",
    "    # print(sentence)\n",
    "    for word_index, word in enumerate(sentence):\n",
    "        for nb_word in sentence[max(word_index - WINDOW_SIZE, 0) : min(word_index + WINDOW_SIZE, len(sentence)) + 1] :    \n",
    "            if nb_word != word:\n",
    "                main_word.append(word)\n",
    "                lb_word.append(nb_word)\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(zip(main_word, lb_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating one layer neural network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "ONE_HOT_DIM = len(unique_words)\n",
    "\n",
    "def get_one_hot(key):\n",
    "    array = [0]*ONE_HOT_DIM\n",
    "    array[word_ids[key]] = 1\n",
    "    return array\n",
    "\n",
    "# print(get_one_hot('egyptian'))\n",
    "\n",
    "X = [] # input word\n",
    "Y = [] # target word\n",
    "\n",
    "for x,y  in zip(main_word, lb_word):\n",
    "    X.append(get_one_hot(x))\n",
    "    Y.append(get_one_hot(y))\n",
    "    \n",
    "X_train = np.asarray(X)\n",
    "Y_train = np.asarray(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making placeholders for X_train and Y_train\n",
    "x = tf.placeholder(tf.float32, shape=(None, ONE_HOT_DIM))\n",
    "y_label = tf.placeholder(tf.float32, shape=(None, ONE_HOT_DIM))\n",
    "\n",
    "# word embedding will be 2 dimension for 2d visualization\n",
    "EMBEDDING_DIM = 3 \n",
    "\n",
    "# hidden layer: which represents word vector eventually\n",
    "W1 = tf.Variable(tf.random_normal([ONE_HOT_DIM, EMBEDDING_DIM]))\n",
    "b1 = tf.Variable(tf.random_normal([1])) #bias\n",
    "hidden_layer = tf.add(tf.matmul(x,W1), b1)\n",
    "\n",
    "# output layer\n",
    "W2 = tf.Variable(tf.random_normal([EMBEDDING_DIM, ONE_HOT_DIM]))\n",
    "b2 = tf.Variable(tf.random_normal([1]))\n",
    "prediction = tf.nn.softmax(tf.add( tf.matmul(hidden_layer, W2), b2))\n",
    "\n",
    "# loss function: cross entropy\n",
    "loss = tf.reduce_mean(-tf.reduce_sum(y_label * tf.log(prediction), axis=[1]))\n",
    "\n",
    "# training operation\n",
    "train_op = tf.train.GradientDescentOptimizer(0.05).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 loss is :  5.1089883\n",
      "iteration 3000 loss is :  1.8507527\n",
      "iteration 6000 loss is :  1.5673333\n",
      "iteration 9000 loss is :  1.5180882\n",
      "iteration 12000 loss is :  1.4942526\n",
      "iteration 15000 loss is :  1.4789653\n",
      "iteration 18000 loss is :  1.4695277\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init) \n",
    "\n",
    "iteration = 20000\n",
    "for i in range(iteration):\n",
    "    # input is X_train which is one hot encoded word\n",
    "    # label is Y_train which is one hot encoded neighbor word\n",
    "    sess.run(train_op, feed_dict={x: X_train, y_label: Y_train})\n",
    "    if i % 3000 == 0:\n",
    "        print('iteration '+str(i)+' loss is : ', sess.run(loss, feed_dict={x: X_train, y_label: Y_train}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.1443722   2.3578405  -1.7974826 ]\n",
      " [ 0.55547386  3.0314193  -2.6923094 ]\n",
      " [-2.5955384   2.1580765   1.7023705 ]\n",
      " [-1.0204542   0.53643495  0.27944535]\n",
      " [-1.0583526   0.7624183  -3.912158  ]\n",
      " [ 2.9373715  -1.7509509   2.3712595 ]\n",
      " [ 1.9486309  -2.5520415   2.0448358 ]\n",
      " [ 2.7526534  -1.7446601  -0.4242946 ]\n",
      " [-1.9991106  -0.21119475  1.9516578 ]\n",
      " [ 3.2658482   0.7630225   0.54451025]\n",
      " [-3.5113933   1.5029639   0.49950525]\n",
      " [ 2.2551913   0.22754255 -3.3623257 ]\n",
      " [ 0.3510653  -2.5054908  -3.4675937 ]\n",
      " [ 3.032895    0.17217594  2.499767  ]\n",
      " [ 0.2246389  -0.99584526 -1.7047981 ]\n",
      " [ 0.8602616  -0.1773966   0.6414633 ]\n",
      " [-0.03150687 -2.4772356   2.5864713 ]\n",
      " [ 0.39474475 -3.683622    0.16904785]\n",
      " [ 2.737373    1.9833935  -2.6902564 ]\n",
      " [ 0.12919845  0.35039297 -2.0579782 ]\n",
      " [ 0.32637963  1.3154236  -1.4105233 ]\n",
      " [-3.8903463  -0.5285146  -0.02569342]\n",
      " [-1.0314983   1.7134744   2.861213  ]\n",
      " [-2.2078302  -0.7023514   3.085438  ]\n",
      " [ 2.0321336   2.055433    1.7341663 ]\n",
      " [ 3.519062    1.5439998   0.13281175]\n",
      " [-1.8638523  -1.0263404  -3.4833283 ]]\n"
     ]
    }
   ],
   "source": [
    "# Now the hidden layer (W1 + b1) is actually the word look up table\n",
    "vectors = sess.run(W1 + b1)\n",
    "print(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py3)",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
